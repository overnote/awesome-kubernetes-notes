

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>六 集群故障管理 &mdash; Zeusro&#39;s awesome-kubernetes-notes 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="七 控制器配置清单" href="awesome-kubernetes-notes.html" />
    <link rel="prev" title="五 配置清单使用" href="chapter_5.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Zeusro's awesome-kubernetes-notes
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chapter_0.html">关于“我”</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_1.html">一 Kubernetes概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_2.html">二 核心组件/附件</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_3.html">三 集群部署</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_4.html">四 入门命令</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_5.html">五 配置清单使用</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">六 集群故障管理</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">6.1 节点问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">6.1.1 删除节点的正确步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">6.1.2 维护节点的正确步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#imagegcfailed">6.1.3  ImageGCFailed</a></li>
<li class="toctree-l3"><a class="reference internal" href="#diskpressure">6.1.4 节点出现磁盘压力(DiskPressure)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cpu">6.1.5 节点CPU彪高</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unknown">6.1.6 节点失联(unknown)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#systemoom">6.1.7 SystemOOM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#docker-daemon">6.1.8 docker daemon 卡住</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pod">6.2 pod</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">6.2.1 pod频繁重启</a></li>
<li class="toctree-l3"><a class="reference internal" href="#limit">6.2.2 资源达到limit设置值</a></li>
<li class="toctree-l3"><a class="reference internal" href="#readiness-liveness-connection-refused">6.2.3 Readiness/Liveness connection refused</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pod-evicted">6.2.4 pod被驱逐(Evicted)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kubectl-exec">6.2.5 kubectl exec 进入容器失败</a></li>
<li class="toctree-l3"><a class="reference internal" href="#podvirtual-host-name">6.2.6 pod的virtual host name</a></li>
<li class="toctree-l3"><a class="reference internal" href="#podcrashbackoff">6.2.7 pod接连Crashbackoff</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dns">6.2.8 DNS 效率低下</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy">6.3 deploy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#minimumreplicationunavailable">6.3.1 MinimumReplicationUnavailable</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#service">6.4 service</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#po">6.4.1 建了一个服务,但是没有对应的po,会出现什么情况?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#service-connection-refuse">6.4.2 service connection refuse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">6.4.3 service没有负载均衡</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#replicationcontroller">6.5 ReplicationController</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">6.5.1 不更新</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#statefulset">6.6 StatefulSet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">6.6.1 pod 更新失败</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unknown-pod">6.6.2 unknown pod</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#hpa">6.7 HPA</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kubernetes">6.8 阿里云Kubernetes问题</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ingress">6.8.1 修改默认ingress</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loadbalancerip">6.8.2 LoadBalancer服务一直没有IP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#statefulsetpvc">6.8.3 清理Statefulset动态PVC</a></li>
<li class="toctree-l3"><a class="reference internal" href="#v1-12-6-aliyun-1">6.8.4 升级到v1.12.6-aliyun.1之后节点可分配内存变少</a></li>
<li class="toctree-l3"><a class="reference internal" href="#networkunavailable">6.8.5 新加节点出现NetworkUnavailable</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loadbalancer-svc">6.8.6 访问LoadBalancer svc随机出现流量转发异常</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">6.8.7 控制台显示的节点内存使用率总是偏大</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ingress-controller">6.8.8 Ingress Controller 玄学优化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pid">6.8.9 pid 问题</a></li>
<li class="toctree-l3"><a class="reference internal" href="#oss-pvc-failedmount">6.8.10 OSS PVC FailedMount</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html">七 控制器配置清单</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#service">八 Service 配置清单</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#ingress">九 ingress 控制器</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#pod">十 POD 存储卷</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id25">十一 配置信息容器化</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#statefulset">十二 StatefulSet 控制器</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id33">十三 用户认证系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id40">十四 用户权限系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#dashboard">十五 dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id46">十六 网络通信</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id55">十七 调度策略</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id62">十八 高级调度设置</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id68">十九 容器资源限制</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#heapster">二十 HeapSter监控（废弃中）</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id71">二十一 新一代监控架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#id75">二十二 K8S包管理器</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#etcd">二十三 ETCD详解</a></li>
<li class="toctree-l1"><a class="reference internal" href="awesome-kubernetes-notes.html#kubesphere">二十四 国产容器管理平台KubeSphere实战排错</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Zeusro's awesome-kubernetes-notes</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>六 集群故障管理</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/chapter_6.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>六 集群故障管理<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>6.1 节点问题<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>6.1.1 删除节点的正确步骤<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># SchedulingDisabled,确保新的容器不会调度到该节点</span>
kubectl cordon <span class="nv">$node</span>
<span class="c1"># 驱逐除了ds以外所有的pod</span>
kubectl drain <span class="nv">$node</span>   --ignore-daemonsets
kubectl delete <span class="nv">$node</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h3>6.1.2 维护节点的正确步骤<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># SchedulingDisabled,确保新的容器不会调度到该节点</span>
kubectl cordon <span class="nv">$node</span>
<span class="c1"># 驱逐除了ds以外所有的pod</span>
kubectl drain <span class="nv">$node</span> --ignore-daemonsets --delete-local-data
<span class="c1"># 维护完成,恢复其正常状态</span>
kubectl uncordon <span class="nv">$node</span>
</pre></div>
</div>
<p>–delete-local-data 是忽略 <code class="docutils literal notranslate"><span class="pre">emptyDir</span></code>这类的临时存储的意思</p>
</div>
<div class="section" id="imagegcfailed">
<h3>6.1.3  ImageGCFailed<a class="headerlink" href="#imagegcfailed" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>kubelet 可以清除未使用的容器和镜像。kubelet 在每分钟和每五分钟分别回收容器和镜像。</p>
<p><a class="reference external" href="https://k8smeetup.github.io/docs/concepts/cluster-administration/kubelet-garbage-collection/">配置 kubelet 垃圾收集</a></p>
</div></blockquote>
<p>但是 kubelet 的垃圾回收有个问题,它只能回收那些未使用的镜像,有点像 <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">system</span> <span class="pre">prune</span></code>,然而观察发现,那些死掉的容器不是最大的问题,正在运行的容器才是更大的问题.如果ImageGCFailed一直发生,而容器使用的ephemeral-storage/hostpath(宿主目录)越发增多,最终将会导致更严重的DiskPressure问题,波及节点上所有容器.</p>
<p>建议:</p>
<ol class="simple">
<li><p>高配机器(4核32G以上)的docker目录配置100G SSD以上空间</p></li>
<li><p>配置<a class="reference external" href="https://kubernetes.io/docs/concepts/policy/resource-quotas/#storage-resource-quota">ResourceQuota</a>限制整体资源限额</p></li>
<li><p>容器端禁用ephemeral-storage(本地文件写入),或者使用spec.containers[].resources.limits.ephemeral-storage限制,控制宿主目录写入</p></li>
</ol>
</div>
<div class="section" id="diskpressure">
<h3>6.1.4 节点出现磁盘压力(DiskPressure)<a class="headerlink" href="#diskpressure" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">eviction</span><span class="o">-</span><span class="n">hard</span><span class="o">=</span><span class="n">imagefs</span><span class="o">.</span><span class="n">available</span><span class="o">&lt;</span><span class="mi">15</span><span class="o">%</span><span class="p">,</span><span class="n">memory</span><span class="o">.</span><span class="n">available</span><span class="o">&lt;</span><span class="mi">300</span><span class="n">Mi</span><span class="p">,</span><span class="n">nodefs</span><span class="o">.</span><span class="n">available</span><span class="o">&lt;</span><span class="mi">10</span><span class="o">%</span><span class="p">,</span><span class="n">nodefs</span><span class="o">.</span><span class="n">inodesFree</span><span class="o">&lt;</span><span class="mi">5</span><span class="o">%</span>
</pre></div>
</div>
<p>kubelet在启动时指定了磁盘压力,以阿里云为例,<code class="docutils literal notranslate"><span class="pre">imagefs.available&lt;15%</span></code>意思是说容器的读写层少于15%的时候,节点会被驱逐.节点被驱逐的后果就是产生DiskPressure这种状况,并且节点上再也不能运行任何镜像,直至磁盘问题得到解决.如果节点上容器使用了宿主目录,这个问题将会是致命的.因为你不能把目录删除掉,但是真是这些宿主机的目录堆积,导致了节点被驱逐.</p>
<p>所以,平时要养好良好习惯,容器里面别瞎写东西(容器里面写文件会占用ephemeral-storage,ephemeral-storage过多pod会被驱逐),多使用无状态型容器,谨慎选择存储方式,尽量别用hostpath这种存储</p>
<p>出现状况时,真的有种欲哭无泪的感觉.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Events</span><span class="p">:</span>
  <span class="n">Type</span>     <span class="n">Reason</span>                 <span class="n">Age</span>                   <span class="n">From</span>                                            <span class="n">Message</span>
  <span class="o">----</span>     <span class="o">------</span>                 <span class="o">----</span>                  <span class="o">----</span>                                            <span class="o">-------</span>
  <span class="ne">Warning</span>  <span class="n">FreeDiskSpaceFailed</span>    <span class="mi">23</span><span class="n">m</span>                   <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">failed</span> <span class="n">to</span> <span class="n">garbage</span> <span class="n">collect</span> <span class="n">required</span> <span class="n">amount</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">Wanted</span> <span class="n">to</span> <span class="n">free</span> <span class="mi">5182058496</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">but</span> <span class="n">freed</span> <span class="mi">0</span> <span class="nb">bytes</span>
  <span class="ne">Warning</span>  <span class="n">FreeDiskSpaceFailed</span>    <span class="mi">18</span><span class="n">m</span>                   <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">failed</span> <span class="n">to</span> <span class="n">garbage</span> <span class="n">collect</span> <span class="n">required</span> <span class="n">amount</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">Wanted</span> <span class="n">to</span> <span class="n">free</span> <span class="mi">6089891840</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">but</span> <span class="n">freed</span> <span class="mi">0</span> <span class="nb">bytes</span>
  <span class="ne">Warning</span>  <span class="n">ImageGCFailed</span>          <span class="mi">18</span><span class="n">m</span>                   <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">failed</span> <span class="n">to</span> <span class="n">garbage</span> <span class="n">collect</span> <span class="n">required</span> <span class="n">amount</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">Wanted</span> <span class="n">to</span> <span class="n">free</span> <span class="mi">6089891840</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">but</span> <span class="n">freed</span> <span class="mi">0</span> <span class="nb">bytes</span>
  <span class="ne">Warning</span>  <span class="n">FreeDiskSpaceFailed</span>    <span class="mi">13</span><span class="n">m</span>                   <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">failed</span> <span class="n">to</span> <span class="n">garbage</span> <span class="n">collect</span> <span class="n">required</span> <span class="n">amount</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">Wanted</span> <span class="n">to</span> <span class="n">free</span> <span class="mi">4953321472</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">but</span> <span class="n">freed</span> <span class="mi">0</span> <span class="nb">bytes</span>
  <span class="ne">Warning</span>  <span class="n">ImageGCFailed</span>          <span class="mi">13</span><span class="n">m</span>                   <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">failed</span> <span class="n">to</span> <span class="n">garbage</span> <span class="n">collect</span> <span class="n">required</span> <span class="n">amount</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">Wanted</span> <span class="n">to</span> <span class="n">free</span> <span class="mi">4953321472</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">but</span> <span class="n">freed</span> <span class="mi">0</span> <span class="nb">bytes</span>
  <span class="n">Normal</span>   <span class="n">NodeHasNoDiskPressure</span>  <span class="mi">10</span><span class="n">m</span> <span class="p">(</span><span class="n">x5</span> <span class="n">over</span> <span class="mi">47</span><span class="n">d</span><span class="p">)</span>     <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">Node</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span> <span class="n">status</span> <span class="ow">is</span> <span class="n">now</span><span class="p">:</span> <span class="n">NodeHasNoDiskPressure</span>
  <span class="n">Normal</span>   <span class="n">Starting</span>               <span class="mi">10</span><span class="n">m</span>                   <span class="n">kube</span><span class="o">-</span><span class="n">proxy</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>  <span class="n">Starting</span> <span class="n">kube</span><span class="o">-</span><span class="n">proxy</span><span class="o">.</span>
  <span class="n">Normal</span>   <span class="n">NodeHasDiskPressure</span>    <span class="mi">10</span><span class="n">m</span> <span class="p">(</span><span class="n">x4</span> <span class="n">over</span> <span class="mi">42</span><span class="n">m</span><span class="p">)</span>     <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">Node</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span> <span class="n">status</span> <span class="ow">is</span> <span class="n">now</span><span class="p">:</span> <span class="n">NodeHasDiskPressure</span>
  <span class="ne">Warning</span>  <span class="n">EvictionThresholdMet</span>   <span class="mi">8</span><span class="n">m29s</span> <span class="p">(</span><span class="n">x19</span> <span class="n">over</span> <span class="mi">42</span><span class="n">m</span><span class="p">)</span>  <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">Attempting</span> <span class="n">to</span> <span class="n">reclaim</span> <span class="n">ephemeral</span><span class="o">-</span><span class="n">storage</span>
  <span class="ne">Warning</span>  <span class="n">ImageGCFailed</span>          <span class="mi">3</span><span class="n">m4s</span>                  <span class="n">kubelet</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">xxxx1</span>     <span class="n">failed</span> <span class="n">to</span> <span class="n">garbage</span> <span class="n">collect</span> <span class="n">required</span> <span class="n">amount</span> <span class="n">of</span> <span class="n">images</span><span class="o">.</span> <span class="n">Wanted</span> <span class="n">to</span> <span class="n">free</span> <span class="mi">4920913920</span> <span class="nb">bytes</span><span class="p">,</span> <span class="n">but</span> <span class="n">freed</span> <span class="mi">0</span> <span class="nb">bytes</span>
</pre></div>
</div>
<p>ImageGCFailed 是很坑爹的状态,出现这个状态时,表示 kubelet 尝试回收磁盘失败,这时得考虑是否要手动上机修复了.</p>
<p>建议:</p>
<ol class="simple">
<li><p>镜像数量在200以上时,采购100G SSD存镜像</p></li>
<li><p>少用临时存储(empty-dir,hostpath之类的)</p></li>
</ol>
<p>参考链接:</p>
<ol class="simple">
<li><p><a class="reference external" href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-signals">Eviction Signals</a></p></li>
<li><p><a class="reference external" href="http://dockone.io/article/783">10张图带你深入理解Docker容器和镜像</a></p></li>
</ol>
</div>
<div class="section" id="cpu">
<h3>6.1.5 节点CPU彪高<a class="headerlink" href="#cpu" title="Permalink to this headline">¶</a></h3>
<p>有可能是节点在进行GC(container GC/image GC),用<code class="docutils literal notranslate"><span class="pre">describe</span> <span class="pre">node</span></code>查查.我有次遇到这种状况,最后节点上的容器少了很多,也是有点郁闷</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Events</span><span class="p">:</span>
  <span class="n">Type</span>     <span class="n">Reason</span>                 <span class="n">Age</span>                 <span class="n">From</span>                                         <span class="n">Message</span>
  <span class="o">----</span>     <span class="o">------</span>                 <span class="o">----</span>                <span class="o">----</span>
  <span class="ne">Warning</span>  <span class="n">ImageGCFailed</span>          <span class="mi">45</span><span class="n">m</span>                 <span class="n">kubelet</span><span class="p">,</span> <span class="n">cn</span><span class="o">-</span><span class="n">shenzhen</span><span class="o">.</span><span class="n">xxxx</span>  <span class="n">failed</span> <span class="n">to</span> <span class="n">get</span> <span class="n">image</span> <span class="n">stats</span><span class="p">:</span> <span class="n">rpc</span> <span class="n">error</span><span class="p">:</span> <span class="n">code</span> <span class="o">=</span> <span class="n">DeadlineExceeded</span> <span class="n">desc</span> <span class="o">=</span> <span class="n">context</span> <span class="n">deadline</span> <span class="n">exceeded</span>
</pre></div>
</div>
<p>参考:</p>
<p><a class="reference external" href="https://cizixs.com/2017/06/09/kubelet-source-code-analysis-part-3/">kubelet 源码分析：Garbage Collect</a></p>
</div>
<div class="section" id="unknown">
<h3>6.1.6 节点失联(unknown)<a class="headerlink" href="#unknown" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">Ready</span>                <span class="kc">False</span>   <span class="n">Fri</span><span class="p">,</span> <span class="mi">28</span> <span class="n">Jun</span> <span class="mi">2019</span> <span class="mi">10</span><span class="p">:</span><span class="mi">19</span><span class="p">:</span><span class="mi">21</span> <span class="o">+</span><span class="mi">0800</span>   <span class="n">Thu</span><span class="p">,</span> <span class="mi">27</span> <span class="n">Jun</span> <span class="mi">2019</span> <span class="mi">07</span><span class="p">:</span><span class="mi">07</span><span class="p">:</span><span class="mi">38</span> <span class="o">+</span><span class="mi">0800</span>   <span class="n">KubeletNotReady</span>              <span class="n">PLEG</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">healthy</span><span class="p">:</span> <span class="n">pleg</span> <span class="n">was</span> <span class="n">last</span> <span class="n">seen</span> <span class="n">active</span> <span class="mi">27</span><span class="n">h14m51</span><span class="o">.</span><span class="mi">413818128</span><span class="n">s</span> <span class="n">ago</span><span class="p">;</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="mi">3</span><span class="n">m0s</span>

<span class="n">Events</span><span class="p">:</span>
  <span class="n">Type</span>     <span class="n">Reason</span>             <span class="n">Age</span>                 <span class="n">From</span>                                         <span class="n">Message</span>
  <span class="o">----</span>     <span class="o">------</span>             <span class="o">----</span>                <span class="o">----</span>                                         <span class="o">-------</span>
  <span class="ne">Warning</span>  <span class="n">ContainerGCFailed</span>  <span class="mi">5</span><span class="n">s</span> <span class="p">(</span><span class="n">x543</span> <span class="n">over</span> <span class="mi">27</span><span class="n">h</span><span class="p">)</span>  <span class="n">kubelet</span><span class="p">,</span> <span class="n">cn</span><span class="o">-</span><span class="n">shenzhen</span><span class="o">.</span><span class="n">xxxx</span>                    <span class="n">rpc</span> <span class="n">error</span><span class="p">:</span> <span class="n">code</span> <span class="o">=</span> <span class="n">DeadlineExceeded</span> <span class="n">desc</span> <span class="o">=</span> <span class="n">context</span> <span class="n">deadline</span> <span class="n">exceeded</span>
</pre></div>
</div>
<p>ssh登录主机后发现,docker服务虽然还在运行,但<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">ps</span></code>卡住了.于是我顺便升级了内核到5.1,然后重启.</p>
<p>后来发现是有个人上了一个问题镜像，无论在哪节点运行，都会把节点搞瘫，也是醉了。</p>
<p>unknown 是非常严重的问题,必须要予以重视.节点出现 unknown ,kubernetes master 自身不知道节点上面的容器是死是活,假如有一个非常重要的容器在 unknown 节点上面运行,而且他刚好又挂了,kubernetes是不会自动帮你另启一个容器的,这点要注意.</p>
<p>参考链接:</p>
<p><a class="reference external" href="https://github.com/kubernetes/kubernetes/issues/45419">Node flapping between Ready/NotReady with PLEG issues</a>
<a class="reference external" href="https://my.oschina.net/jxcdwangtao/blog/1594348">深度解析Kubernetes Pod Disruption Budgets(PDB)</a></p>
</div>
<div class="section" id="systemoom">
<h3>6.1.7 SystemOOM<a class="headerlink" href="#systemoom" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">SystemOOM</span></code> 并不一定是机器内存用完了.有一种情况是docker 在控制容器的内存导致的.</p>
<p>默认情况下Docker的存放位置为：/var/lib/docker/containers/$id</p>
<p>这个目录下面有个重要的文件: <code class="docutils literal notranslate"><span class="pre">hostconfig.json</span></code>,截取部分大概长这样:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>	&quot;MemorySwappiness&quot;: -1,
	&quot;OomKillDisable&quot;: false,
	&quot;PidsLimit&quot;: 0,
	&quot;Ulimits&quot;: null,
	&quot;CpuCount&quot;: 0,
	&quot;CpuPercent&quot;: 0,
	&quot;IOMaximumIOps&quot;: 0,
	&quot;IOMaximumBandwidth&quot;: 0
}
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&quot;OomKillDisable&quot;:</span> <span class="pre">false,</span></code> 禁止了 docker 服务通过杀进程/重启的方式去和谐使用资源超限的容器,而是以其他的方式去制裁(具体的可以看<a class="reference external" href="https://docs.docker.com/config/containers/resource_constraints/">这里</a>)</p>
</div>
<div class="section" id="docker-daemon">
<h3>6.1.8 docker daemon 卡住<a class="headerlink" href="#docker-daemon" title="Permalink to this headline">¶</a></h3>
<p>这种状况我出现过一次,原因是某个容器有毛病,坑了整个节点.</p>
<p>出现这个问题要尽快解决,因为节点上面所有的 pod 都会变成 unknown .</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>systemctl daemon-reexec
systemctl restart docker<span class="o">(</span>可选视情况定<span class="o">)</span>
systemctl restart kubelet
</pre></div>
</div>
<p>严重时只能重启节点,停止涉事容器.</p>
<p>建议: <code class="docutils literal notranslate"><span class="pre">对于容器的liveness/readiness</span> <span class="pre">使用tcp/httpget的方式，避免</span> <span class="pre">高频率使用exec</span></code></p>
</div>
</div>
<div class="section" id="pod">
<h2>6.2 pod<a class="headerlink" href="#pod" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id5">
<h3>6.2.1 pod频繁重启<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>原因有多种,不可一概而论</p>
<p>有一种情况是,deploy配置了健康检查,节点运行正常,但是因为节点负载过高导致了健康检查失败(load15长期大于2以上),频繁Backoff.我调高了不健康阈值之后,降低节点负载之后,问题解决</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span>          <span class="nt">livenessProbe</span><span class="p">:</span>
            <span class="c1"># 不健康阈值</span>
            <span class="nt">failureThreshold</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
            <span class="nt">initialDelaySeconds</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>
            <span class="nt">periodSeconds</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10</span>
            <span class="nt">successThreshold</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
            <span class="nt">tcpSocket</span><span class="p">:</span>
              <span class="nt">port</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">8080</span>
            <span class="nt">timeoutSeconds</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
</div>
<div class="section" id="limit">
<h3>6.2.2 资源达到limit设置值<a class="headerlink" href="#limit" title="Permalink to this headline">¶</a></h3>
<p>调高limit或者检查应用</p>
</div>
<div class="section" id="readiness-liveness-connection-refused">
<h3>6.2.3 Readiness/Liveness connection refused<a class="headerlink" href="#readiness-liveness-connection-refused" title="Permalink to this headline">¶</a></h3>
<p>Readiness检查失败的也会重启,但是<code class="docutils literal notranslate"><span class="pre">Readiness</span></code>检查失败不一定是应用的问题,如果节点本身负载过重,也是会出现connection refused或者timeout</p>
<p>这个问题要上节点排查</p>
</div>
<div class="section" id="pod-evicted">
<h3>6.2.4 pod被驱逐(Evicted)<a class="headerlink" href="#pod-evicted" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>节点加了污点导致pod被驱逐</p></li>
<li><p>ephemeral-storage超过限制被驱逐</p>
<ol class="simple">
<li><p>EmptyDir 的使用量超过了他的 SizeLimit，那么这个 pod 将会被驱逐</p></li>
<li><p>Container 的使用量（log，如果没有 overlay 分区，则包括 imagefs）超过了他的 limit，则这个 pod 会被驱逐</p></li>
<li><p>Pod 对本地临时存储总的使用量（所有 emptydir 和 container）超过了 pod 中所有container 的 limit 之和，则 pod 被驱逐</p></li>
</ol>
</li>
</ol>
<p>ephemeral-storage是一个pod用的临时存储.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">resources</span><span class="p">:</span>
       <span class="n">requests</span><span class="p">:</span> 
           <span class="n">ephemeral</span><span class="o">-</span><span class="n">storage</span><span class="p">:</span> <span class="s2">&quot;2Gi&quot;</span>
       <span class="n">limits</span><span class="p">:</span>
           <span class="n">ephemeral</span><span class="o">-</span><span class="n">storage</span><span class="p">:</span> <span class="s2">&quot;3Gi&quot;</span>
</pre></div>
</div>
<p>节点被驱逐后通过get po还是能看到,用describe命令,可以看到被驱逐的历史原因</p>
<blockquote>
<div><p>Message:            The node was low on resource: ephemeral-storage. Container codis-proxy was using 10619440Ki, which exceeds its request of 0.</p>
</div></blockquote>
<p>参考:</p>
<ol class="simple">
<li><p><a class="reference external" href="https://blog.csdn.net/hyneria_hope/article/details/79467922">Kubernetes pod ephemeral-storage配置</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">Managing Compute Resources for Containers</a></p></li>
</ol>
</div>
<div class="section" id="kubectl-exec">
<h3>6.2.5 kubectl exec 进入容器失败<a class="headerlink" href="#kubectl-exec" title="Permalink to this headline">¶</a></h3>
<p>这种问题我在搭建codis-server的时候遇到过,当时没有配置就绪以及健康检查.但获取pod描述的时候,显示running.其实这个时候容器以及不正常了.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">~</span> <span class="n">kex</span> <span class="n">codis</span><span class="o">-</span><span class="n">server</span><span class="o">-</span><span class="mi">3</span> <span class="n">sh</span>
<span class="n">rpc</span> <span class="n">error</span><span class="p">:</span> <span class="n">code</span> <span class="o">=</span> <span class="mi">2</span> <span class="n">desc</span> <span class="o">=</span> <span class="n">containerd</span><span class="p">:</span> <span class="n">container</span> <span class="ow">not</span> <span class="n">found</span>
<span class="n">command</span> <span class="n">terminated</span> <span class="k">with</span> <span class="n">exit</span> <span class="n">code</span> <span class="mi">126</span>
</pre></div>
</div>
<p>解决办法:删了这个pod,配置<code class="docutils literal notranslate"><span class="pre">livenessProbe</span></code></p>
</div>
<div class="section" id="podvirtual-host-name">
<h3>6.2.6 pod的virtual host name<a class="headerlink" href="#podvirtual-host-name" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Deployment</span></code>衍生的pod,<code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">host</span> <span class="pre">name</span></code>就是<code class="docutils literal notranslate"><span class="pre">pod</span> <span class="pre">name</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">StatefulSet</span></code>衍生的pod,<code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">host</span> <span class="pre">name</span></code>是<code class="docutils literal notranslate"><span class="pre">&lt;pod</span> <span class="pre">name&gt;.&lt;svc</span> <span class="pre">name&gt;.&lt;namespace&gt;.svc.cluster.local</span></code>.相比<code class="docutils literal notranslate"><span class="pre">Deployment</span></code>显得更有规律一些.而且支持其他pod访问</p>
</div>
<div class="section" id="podcrashbackoff">
<h3>6.2.7 pod接连Crashbackoff<a class="headerlink" href="#podcrashbackoff" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Crashbackoff</span></code>有多种原因.</p>
<p>沙箱创建(FailedCreateSandBox)失败,多半是cni网络插件的问题</p>
<p>镜像拉取,有中国特色社会主义的问题,可能太大了,拉取较慢</p>
<p>也有一种可能是容器并发过高,流量雪崩导致.</p>
<p>比如,现在有3个容器abc,a突然遇到流量洪峰导致内部奔溃,继而<code class="docutils literal notranslate"><span class="pre">Crashbackoff</span></code>,那么a就会被<code class="docutils literal notranslate"><span class="pre">service</span></code>剔除出去,剩下的bc也承载不了那么多流量,接连崩溃,最终网站不可访问.这种情况,多见于高并发网站+低效率web容器.</p>
<p>在不改变代码的情况下,最优解是增加副本数,并且加上hpa,实现动态伸缩容.</p>
</div>
<div class="section" id="dns">
<h3>6.2.8 DNS 效率低下<a class="headerlink" href="#dns" title="Permalink to this headline">¶</a></h3>
<p>容器内打开nscd(域名缓存服务)，可大幅提升解析性能</p>
<p>严禁生产环境使用alpine作为基础镜像(会导致dns解析请求异常)</p>
</div>
</div>
<div class="section" id="deploy">
<h2>6.3 deploy<a class="headerlink" href="#deploy" title="Permalink to this headline">¶</a></h2>
<div class="section" id="minimumreplicationunavailable">
<h3>6.3.1 MinimumReplicationUnavailable<a class="headerlink" href="#minimumreplicationunavailable" title="Permalink to this headline">¶</a></h3>
<p>如果<code class="docutils literal notranslate"><span class="pre">deploy</span></code>配置了SecurityContext,但是api-server拒绝了,就会出现这个情况,在api-server的容器里面,去掉<code class="docutils literal notranslate"><span class="pre">SecurityContextDeny</span></code>这个启动参数.</p>
<p>具体见<a class="reference external" href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Using Admission Controllers</a></p>
</div>
</div>
<div class="section" id="service">
<h2>6.4 service<a class="headerlink" href="#service" title="Permalink to this headline">¶</a></h2>
<div class="section" id="po">
<h3>6.4.1 建了一个服务,但是没有对应的po,会出现什么情况?<a class="headerlink" href="#po" title="Permalink to this headline">¶</a></h3>
<p>请求时一直不会有响应,直到request timeout</p>
<p>参考</p>
<ol class="simple">
<li><p><a class="reference external" href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#node-conditions">Configure Out Of Resource Handling</a></p></li>
</ol>
</div>
<div class="section" id="service-connection-refuse">
<h3>6.4.2 service connection refuse<a class="headerlink" href="#service-connection-refuse" title="Permalink to this headline">¶</a></h3>
<p>原因可能有</p>
<ol class="simple">
<li><p>pod没有设置readinessProbe,请求到未就绪的pod</p></li>
<li><p>kube-proxy宕机了(kube-proxy负责转发请求)</p></li>
<li><p>网络过载</p></li>
</ol>
</div>
<div class="section" id="id6">
<h3>6.4.3 service没有负载均衡<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>检查一下是否用了<code class="docutils literal notranslate"><span class="pre">headless</span> <span class="pre">service</span></code>.<code class="docutils literal notranslate"><span class="pre">headless</span> <span class="pre">service</span></code>是不会自动负载均衡的…</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Service</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="c1"># clusterIP: None的即为`headless service`</span>
  <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ClusterIP</span>
  <span class="nt">clusterIP</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">None</span>
</pre></div>
</div>
<p>具体表现service没有自己的虚拟IP,nslookup会出现所有pod的ip.但是ping的时候只会出现第一个pod的ip</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/ # nslookup consul
nslookup: can&#39;t resolve &#39;(null)&#39;: Name does not resolve

Name:      consul
Address 1: 172.31.10.94 172-31-10-94.consul.default.svc.cluster.local
Address 2: 172.31.10.95 172-31-10-95.consul.default.svc.cluster.local
Address 3: 172.31.11.176 172-31-11-176.consul.default.svc.cluster.local

/ # ping consul
PING consul (172.31.10.94): 56 data bytes
64 bytes from 172.31.10.94: seq=0 ttl=62 time=0.973 ms
64 bytes from 172.31.10.94: seq=1 ttl=62 time=0.170 ms
^C
--- consul ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.170/0.571/0.973 ms

/ # ping consul
PING consul (172.31.10.94): 56 data bytes
64 bytes from 172.31.10.94: seq=0 ttl=62 time=0.206 ms
64 bytes from 172.31.10.94: seq=1 ttl=62 time=0.178 ms
^C
--- consul ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.178/0.192/0.206 ms
</pre></div>
</div>
<p>普通的type: ClusterIP service,nslookup会出现该服务自己的IP</p>
<div class="highlight-BASH notranslate"><div class="highlight"><pre><span></span>/ # nslookup consul
nslookup: can&#39;t resolve &#39;(null)&#39;: Name does not resolve

Name:      consul
Address 1: 172.30.15.52 consul.default.svc.cluster.local
</pre></div>
</div>
</div>
</div>
<div class="section" id="replicationcontroller">
<h2>6.5 ReplicationController<a class="headerlink" href="#replicationcontroller" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3>6.5.1 不更新<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>ReplicationController不是用apply去更新的,而是<code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">rolling-update</span></code>,但是这个指令也废除了,取而代之的是<code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">rollout</span></code>.所以应该使用<code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">rollout</span></code>作为更新手段,或者懒一点,apply file之后,delete po.</p>
<p>尽量使用deploy吧.</p>
</div>
</div>
<div class="section" id="statefulset">
<h2>6.6 StatefulSet<a class="headerlink" href="#statefulset" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id8">
<h3>6.6.1 pod 更新失败<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>StatefulSet是逐一更新的,观察一下是否有<code class="docutils literal notranslate"><span class="pre">Crashbackoff</span></code>的容器,有可能是这个容器导致更新卡住了,删掉即可.</p>
</div>
<div class="section" id="unknown-pod">
<h3>6.6.2 unknown pod<a class="headerlink" href="#unknown-pod" title="Permalink to this headline">¶</a></h3>
<p>如果 StatefulSet 绑定 pod 状态变成 unknown ,这个时候是非常坑爹的,StatefulSet不会帮你重建pod.</p>
<p>这时会导致外部请求一直失败.</p>
<p>综合建议,不用 <code class="docutils literal notranslate"><span class="pre">StatefulSet</span></code> ,改用 operator 模式替换它.</p>
</div>
</div>
<div class="section" id="hpa">
<h2>6.7 HPA<a class="headerlink" href="#hpa" title="Permalink to this headline">¶</a></h2>
<p>HPA Controller会通过调整副本数量使得CPU使用率尽量向期望值靠近，而且不是完全相等．另外，官方考虑到自动扩展的决策可能需要一段时间才会生效：例如当pod所需要的CPU负荷过大，从而在创建一个新pod的过程中，系统的CPU使用量可能会同样在有一个攀升的过程。所以，在每一次作出决策后的一段时间内，将不再进行扩展决策。对于扩容而言，这个时间段为3分钟，缩容为5分钟。</p>
<p>HPA Controller中有一个tolerance（容忍力）的概念，它允许一定范围内的使用量的不稳定，现在默认为0.1，这也是出于维护系统稳定性的考虑。例如，设定HPA调度策略为cpu使用率高于50%触发扩容，那么只有当使用率大于55%或者小于45%才会触发伸缩活动，HPA会尽力把Pod的使用率控制在这个范围之间。</p>
<p>具体的每次扩容或者缩容的多少Pod的算法为：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="n">Ceil</span><span class="p">(</span><span class="n">前采集到的使用率</span> <span class="o">/</span> <span class="n">用户自定义的使用率</span><span class="p">)</span> <span class="o">*</span> <span class="n">Pod数量</span><span class="p">)</span>
</pre></div>
</div>
<p>每次最大扩容pod数量不会超过当前副本数量的2倍</p>
<p>参考链接：
<a class="reference external" href="https://cloud.tencent.com/developer/article/1005406">Kubernetes 中 Pod 弹性伸缩详解与使用</a></p>
</div>
<div class="section" id="kubernetes">
<h2>6.8 阿里云Kubernetes问题<a class="headerlink" href="#kubernetes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ingress">
<h3>6.8.1 修改默认ingress<a class="headerlink" href="#ingress" title="Permalink to this headline">¶</a></h3>
<p>新建一个指向ingress的负载均衡型svc,然后修改一下<code class="docutils literal notranslate"><span class="pre">kube-system</span></code>下<code class="docutils literal notranslate"><span class="pre">nginx-ingress-controller</span></code>启动参数.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        <span class="o">-</span> <span class="n">args</span><span class="p">:</span>
            <span class="o">-</span> <span class="o">/</span><span class="n">nginx</span><span class="o">-</span><span class="n">ingress</span><span class="o">-</span><span class="n">controller</span>
            <span class="o">-</span> <span class="s1">&#39;--configmap=$(POD_NAMESPACE)/nginx-configuration&#39;</span>
            <span class="o">-</span> <span class="s1">&#39;--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services&#39;</span>
            <span class="o">-</span> <span class="s1">&#39;--udp-services-configmap=$(POD_NAMESPACE)/udp-services&#39;</span>
            <span class="o">-</span> <span class="s1">&#39;--annotations-prefix=nginx.ingress.kubernetes.io&#39;</span>
            <span class="o">-</span> <span class="s1">&#39;--publish-service=$(POD_NAMESPACE)/&lt;自定义svc&gt;&#39;</span>
            <span class="o">-</span> <span class="s1">&#39;--v=2&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="loadbalancerip">
<h3>6.8.2 LoadBalancer服务一直没有IP<a class="headerlink" href="#loadbalancerip" title="Permalink to this headline">¶</a></h3>
<p>具体表现是EXTERNAL-IP一直显示pending.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>~ kg svc consul-web
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>         AGE
consul-web   LoadBalancer   <span class="m">172</span>.30.13.122   &lt;pending&gt;     <span class="m">443</span>:32082/TCP   5m  
</pre></div>
</div>
<p>这问题跟<a class="reference external" href="https://yq.aliyun.com/articles/626066">Alibaba Cloud Provider</a>这个组件有关,<code class="docutils literal notranslate"><span class="pre">cloud-controller-manager</span></code>有3个组件,他们需要内部选主,可能哪里出错了,当时我把其中一个出问题的<code class="docutils literal notranslate"><span class="pre">pod</span></code>删了,就好了.</p>
</div>
<div class="section" id="statefulsetpvc">
<h3>6.8.3 清理Statefulset动态PVC<a class="headerlink" href="#statefulsetpvc" title="Permalink to this headline">¶</a></h3>
<p>目前阿里云<code class="docutils literal notranslate"><span class="pre">Statefulset</span></code>动态PVC用的是nas。</p>
<ol class="simple">
<li><p>对于这种存储，需要先把容器副本将为0，或者整个<code class="docutils literal notranslate"><span class="pre">Statefulset</span></code>删除。</p></li>
<li><p>删除PVC</p></li>
<li><p>把nas挂载到任意一台服务器上面，然后删除pvc对应nas的目录。</p></li>
</ol>
</div>
<div class="section" id="v1-12-6-aliyun-1">
<h3>6.8.4 升级到v1.12.6-aliyun.1之后节点可分配内存变少<a class="headerlink" href="#v1-12-6-aliyun-1" title="Permalink to this headline">¶</a></h3>
<p>该版本每个节点保留了1Gi,相当于整个集群少了N GB(N为节点数)供Pod分配.</p>
<p>如果节点是4G的,Pod请求3G,极其容易被驱逐.</p>
<p>建议提高节点规格.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Server</span> <span class="n">Version</span><span class="p">:</span> <span class="n">version</span><span class="o">.</span><span class="n">Info</span><span class="p">{</span><span class="n">Major</span><span class="p">:</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="n">Minor</span><span class="p">:</span><span class="s2">&quot;12+&quot;</span><span class="p">,</span> <span class="n">GitVersion</span><span class="p">:</span><span class="s2">&quot;v1.12.6-aliyun.1&quot;</span><span class="p">,</span> <span class="n">GitCommit</span><span class="p">:</span><span class="s2">&quot;8cb561c&quot;</span><span class="p">,</span> <span class="n">GitTreeState</span><span class="p">:</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">BuildDate</span><span class="p">:</span><span class="s2">&quot;2019-04-22T11:34:20Z&quot;</span><span class="p">,</span> <span class="n">GoVersion</span><span class="p">:</span><span class="s2">&quot;go1.10.8&quot;</span><span class="p">,</span> <span class="n">Compiler</span><span class="p">:</span><span class="s2">&quot;gc&quot;</span><span class="p">,</span> <span class="n">Platform</span><span class="p">:</span><span class="s2">&quot;linux/amd64&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="networkunavailable">
<h3>6.8.5 新加节点出现NetworkUnavailable<a class="headerlink" href="#networkunavailable" title="Permalink to this headline">¶</a></h3>
<p>RouteController failed to create a route</p>
<p>看一下kubernetes events,是否出现了</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">timed</span> <span class="n">out</span> <span class="n">waiting</span> <span class="k">for</span> <span class="n">the</span> <span class="n">condition</span> <span class="o">-&gt;</span> <span class="n">WaitCreate</span><span class="p">:</span> <span class="n">ceate</span> <span class="n">route</span> <span class="k">for</span> <span class="n">table</span> <span class="n">vtb</span><span class="o">-</span><span class="n">wz9cpnsbt11hlelpoq2zh</span> <span class="n">error</span><span class="p">,</span> <span class="n">Aliyun</span> <span class="n">API</span> <span class="n">Error</span><span class="p">:</span> <span class="n">RequestId</span><span class="p">:</span> <span class="mi">7006</span><span class="n">BF4E</span><span class="o">-</span><span class="mi">000</span><span class="n">B</span><span class="o">-</span><span class="mf">4E12</span><span class="o">-</span><span class="mi">89</span><span class="n">F2</span><span class="o">-</span><span class="n">F0149D6688E4</span> <span class="n">Status</span> <span class="n">Code</span><span class="p">:</span> <span class="mi">400</span> <span class="n">Code</span><span class="p">:</span> <span class="n">QuotaExceeded</span> <span class="n">Message</span><span class="p">:</span> <span class="n">Route</span> <span class="n">entry</span> <span class="n">quota</span> <span class="n">exceeded</span> <span class="ow">in</span> <span class="n">this</span> <span class="n">route</span> <span class="n">table</span>  
</pre></div>
</div>
<p>出现这个问题是因为达到了<a class="reference external" href="https://help.aliyun.com/document_detail/27750.html">VPC的自定义路由条目限制</a>,默认是48,需要提高<code class="docutils literal notranslate"><span class="pre">vpc_quota_route_entrys_num</span></code>的配额</p>
</div>
<div class="section" id="loadbalancer-svc">
<h3>6.8.6 访问LoadBalancer svc随机出现流量转发异常<a class="headerlink" href="#loadbalancer-svc" title="Permalink to this headline">¶</a></h3>
<p>见
<a class="reference external" href="https://github.com/kubernetes/cloud-provider-alibaba-cloud/issues/57">[bug]阿里云kubernetes版不检查loadbalancer service port,导致流量被异常转发</a>
简单的说，同SLB不能有相同的svc端口，不然会瞎转发。</p>
<p>官方说法：</p>
<blockquote>
<div><p>复用同一个SLB的多个Service不能有相同的前端监听端口，否则会造成端口冲突。</p>
</div></blockquote>
</div>
<div class="section" id="id9">
<h3>6.8.7 控制台显示的节点内存使用率总是偏大<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://xuxinkun.github.io/2016/05/16/memory-monitor-with-cgroup/">Docker容器内存监控</a></p>
<p>原因在于他们控制台用的是usage_in_bytes(cache+buffer),所以会比云监控看到的数字大</p>
</div>
<div class="section" id="ingress-controller">
<h3>6.8.8 Ingress Controller 玄学优化<a class="headerlink" href="#ingress-controller" title="Permalink to this headline">¶</a></h3>
<p>修改 kube-system 下面名为 nginx-configuration 的configmap</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">proxy</span><span class="o">-</span><span class="n">connect</span><span class="o">-</span><span class="n">timeout</span><span class="p">:</span> <span class="s2">&quot;75&quot;</span> 
<span class="n">proxy</span><span class="o">-</span><span class="n">read</span><span class="o">-</span><span class="n">timeout</span><span class="p">:</span> <span class="s2">&quot;75&quot;</span> 
<span class="n">proxy</span><span class="o">-</span><span class="n">send</span><span class="o">-</span><span class="n">timeout</span><span class="p">:</span> <span class="s2">&quot;75&quot;</span> 
<span class="n">upstream</span><span class="o">-</span><span class="n">keepalive</span><span class="o">-</span><span class="n">connections</span><span class="p">:</span> <span class="s2">&quot;300&quot;</span> 
<span class="n">upstream</span><span class="o">-</span><span class="n">keepalive</span><span class="o">-</span><span class="n">timeout</span><span class="p">:</span> <span class="s2">&quot;300&quot;</span> 
<span class="n">upstream</span><span class="o">-</span><span class="n">keepalive</span><span class="o">-</span><span class="n">requests</span><span class="p">:</span> <span class="s2">&quot;1000&quot;</span> 
<span class="n">keep</span><span class="o">-</span><span class="n">alive</span><span class="o">-</span><span class="n">requests</span><span class="p">:</span> <span class="s2">&quot;1000&quot;</span> 
<span class="n">keep</span><span class="o">-</span><span class="n">alive</span><span class="p">:</span> <span class="s2">&quot;300&quot;</span>
<span class="n">disable</span><span class="o">-</span><span class="n">access</span><span class="o">-</span><span class="n">log</span><span class="p">:</span> <span class="s2">&quot;true&quot;</span> 
<span class="n">client</span><span class="o">-</span><span class="n">header</span><span class="o">-</span><span class="n">timeout</span><span class="p">:</span> <span class="s2">&quot;75&quot;</span> 
<span class="n">worker</span><span class="o">-</span><span class="n">processes</span><span class="p">:</span> <span class="s2">&quot;16&quot;</span>
</pre></div>
</div>
<p>注意,是一个项对应一个配置,而不是一个文件. 格式大概这样</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>➜  ~ kg cm nginx-configuration -o yaml
apiVersion: v1
data:
  disable-access-log: &quot;true&quot;
  keep-alive: &quot;300&quot;
  keep-alive-requests: &quot;1000&quot;
  proxy-body-size: 20m
  worker-processes: &quot;16&quot;
  ......
</pre></div>
</div>
</div>
<div class="section" id="pid">
<h3>6.8.9 pid 问题<a class="headerlink" href="#pid" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Message</span><span class="p">:</span> <span class="o">**</span><span class="n">Liveness</span> <span class="n">probe</span> <span class="n">failed</span><span class="p">:</span> <span class="n">rpc</span> <span class="n">error</span><span class="p">:</span> <span class="n">code</span> <span class="o">=</span> <span class="mi">2</span> <span class="n">desc</span> <span class="o">=</span> <span class="n">oci</span> <span class="n">runtime</span> <span class="n">error</span><span class="p">:</span> <span class="n">exec</span> <span class="n">failed</span><span class="p">:</span> <span class="n">container_linux</span><span class="o">.</span><span class="n">go</span><span class="p">:</span><span class="mi">262</span><span class="p">:</span> <span class="n">starting</span> <span class="n">container</span> <span class="n">process</span> <span class="n">caused</span> <span class="s2">&quot;process_linux.go:86: adding pid 30968 to cgroups caused </span><span class="se">\&quot;</span><span class="s2">failed to write 30968 to cgroup.procs: write /sys/fs/cgroup/cpu,cpuacct/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podfe4cc065_cc58_11e9_bf64_00163e08cd06.slice/docker-0447a362d2cf4719ae2a4f5ad0f96f702aacf8ee38d1c73b445ce41bdaa8d24a.scope/cgroup.procs: invalid argument</span><span class="se">\&quot;</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>阿里云初始化节点用的 centos 版本老旧,内核是3.1, Centos7.4的内核3.10还没有支持cgroup对于pid/fd限制,所以会出现这类问题.</p>
<p>建议:</p>
<ol class="simple">
<li><p>手动维护节点,升级到5.x的内核(目前已有一些节点升级到5.x,但是docker版本还是 17.6.2 ,持续观察中~)</p></li>
<li><p>安装 <a class="reference external" href="https://github.com/AliyunContainerService/node-problem-detector">NPD</a> + <a class="reference external" href="https://github.com/AliyunContainerService/kube-eventer">eventer</a> ,利用事件机制提醒管理员手动维护</p></li>
</ol>
</div>
<div class="section" id="oss-pvc-failedmount">
<h3>6.8.10 OSS PVC FailedMount<a class="headerlink" href="#oss-pvc-failedmount" title="Permalink to this headline">¶</a></h3>
<p>可以通过PV制定access key,access secret +PVC的方式使用OSS.某天某个deploy遇到 FailedMount 的问题,联系到阿里云的开发工程师,说是 flexvolume 在初次运行的节点上面运行会有问题,要让他”重新注册”</p>
<p>影响到的版本: registry-vpc.cn-shenzhen.aliyuncs.com/acs/flexvolume:v1.12.6.16-1f4c6cb-aliyun</p>
<p>解决方案:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>touch /usr/libexec/kubernetes/kubelet-plugins/volume/exec/alicloud~oss/debug
</pre></div>
</div>
<p>参考(应用调度相关):</p>
<ol class="simple">
<li><p><a class="reference external" href="http://dockone.io/article/2587">Kubernetes之健康检查与服务依赖处理</a></p></li>
<li><p><a class="reference external" href="https://ieevee.com/tech/2017/04/23/k8s-svc-dependency.html">kubernetes如何解决服务依赖呢？</a></p></li>
<li><p><a class="reference external" href="https://yq.aliyun.com/articles/562440?spm=a2c4e.11153959.0.0.5e0ed55aq1betz">Kubernetes之路 1 - Java应用资源限制的迷思</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies">Control CPU Management Policies on the Node</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons</a></p></li>
<li><p><a class="reference external" href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">Configure Out Of Resource Handling</a></p></li>
</ol>
<p><strong>满意请打赏</strong></p>
<p><img alt="微信支付宝合一" src="_images/zeusro.jpg" /></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="awesome-kubernetes-notes.html" class="btn btn-neutral float-right" title="七 控制器配置清单" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="chapter_5.html" class="btn btn-neutral float-left" title="五 配置清单使用" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, kaliarch,Zeusro

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>